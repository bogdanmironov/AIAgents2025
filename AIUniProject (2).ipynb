{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Preparing data"
      ],
      "metadata": {
        "id": "HLnzF8YGZstt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYptC6tSwJg0",
        "outputId": "e17eba66-beb4-4a88-d918-72a028e4dc52"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "YBIhDRETFrmG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from io import StringIO\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_csv(url, sep=\",\", header=0):\n",
        "    \"\"\"\n",
        "    Downloads a CSV (or TSV) from a given URL and returns a pandas DataFrame.\n",
        "    \"\"\"\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    csv_data = StringIO(response.text)\n",
        "    return pd.read_csv(csv_data, sep=sep, header=header)\n"
      ],
      "metadata": {
        "id": "c61NHGOLH9XY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @inproceedings{chen-etal-2020-low-resource,\n",
        "#     title={Low-Resource Domain Adaptation for Compositional Task-Oriented\n",
        "#         Semantic Parsing},\n",
        "#     author={Xilun Chen and Asish Ghoshal and Yashar Mehdad and Luke Zettlemoyer\n",
        "#         and Sonal Gupta},\n",
        "#     booktitle={Proceedings of the 2020 Conference on Empirical Methods in\n",
        "#         Natural Language Processing (EMNLP)},\n",
        "#     year={2020},\n",
        "#     publisher = \"Association for Computational Linguistics\"\n",
        "# }\n",
        "def load_topv2(url):\n",
        "    \"\"\"\n",
        "    'напомняне' (reminder); 'събитие' (event)\n",
        "    \"\"\"\n",
        "    splits = {\n",
        "        'train': 'data/train-00000-of-00001-4f5cf905029cbf9d.parquet',\n",
        "        'test': 'data/test-00000-of-00001-deac2888ce8ad39d.parquet',\n",
        "        'eval': 'data/eval-00000-of-00001-3ffa52405fac46ab.parquet'\n",
        "    }\n",
        "    full_path = 'hf://datasets/WillHeld/top_v2/' + splits['train']\n",
        "    df = pd.read_parquet(full_path)\n",
        "\n",
        "    def map_domain(domain):\n",
        "        domain = str(domain).strip().lower()\n",
        "        if domain == \"reminder\":\n",
        "            return \"напомняне\"\n",
        "        elif domain == \"event\":\n",
        "            return \"събитие\"\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    df['label'] = df['domain'].apply(map_domain)\n",
        "    df = df.dropna(subset=['label'])\n",
        "    df['text'] = df['utterance']\n",
        "    return df[['text', 'label']]"
      ],
      "metadata": {
        "id": "a4rvd6MXH__6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @article{jauhar2021mslatte,\n",
        "#       title={MS-LaTTE: A Dataset of Where and When To-do Tasks are Completed},\n",
        "#       author={Jauhar, Sujay Kumar and Chandrasekaran, Nirupama and Gamon, Michael and White, Ryen W.},\n",
        "#       journal={arXiv preprint 2111.06902},\n",
        "#       year={2021}\n",
        "# }\n",
        "def load_mslatte(json):\n",
        "    \"\"\"\n",
        "    task to \"задача\".\n",
        "    \"\"\"\n",
        "    df = pd.read_json(json)\n",
        "    df['label'] = \"задача\"\n",
        "    df = df.rename(columns={'TaskTitle': 'text'})\n",
        "    return df[['text', 'label']]\n"
      ],
      "metadata": {
        "id": "7lgYVLNZIEUq"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_event_detection(csv):\n",
        "    \"\"\"\n",
        "    news -> събитие.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(csv)\n",
        "    df['label'] = \"събитие\"\n",
        "    df = df.rename(columns={'Event Name': 'text'})\n",
        "    return df[['text', 'label']]"
      ],
      "metadata": {
        "id": "9gVBeP1FIH-_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @inproceedings{schler2006effects,\n",
        "#     title={Effects of age and gender on blogging.},\n",
        "#     author={Schler, Jonathan and Koppel, Moshe and Argamon, Shlomo and Pennebaker, James W},\n",
        "#     booktitle={AAAI spring symposium: Computational approaches to analyzing weblogs},\n",
        "#     volume={6},\n",
        "#     pages={199--205},\n",
        "#     year={2006}\n",
        "# }\n",
        "def load_notes(url):\n",
        "    \"\"\"\n",
        "    note_text -> бележка.\n",
        "    \"\"\"\n",
        "    dataset = load_dataset(\"barilan/blog_authorship_corpus\")\n",
        "    df = dataset[\"train\"].to_pandas()\n",
        "    df['label'] = \"бележка\"\n",
        "    df = df.rename(columns={'content': 'text'})\n",
        "    return df[['text', 'label']]"
      ],
      "metadata": {
        "id": "NDERNveYIKTs"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topv2_url = \"https://huggingface.co/datasets/WillHeld/top_v2/resolve/main/reminder_split.tsv\"\n",
        "mslatte_url = '/content/drive/MyDrive/Colab Notebooks/data/MS-LaTTE.json'\n",
        "event_detection_url = '/content/drive/MyDrive/Colab Notebooks/data/whats-happening-la-calendar-dataset.csv'\n",
        "notes_url = \"https://huggingface.co/datasets/barilan/blog_authorship_corpus/resolve/main/blog_authorship_corpus.csv\"\n",
        "\n",
        "df_topv2 = load_topv2(topv2_url)\n",
        "df_mslatte = load_mslatte(mslatte_url)\n",
        "df_event = load_event_detection(event_detection_url)\n",
        "df_notes = load_notes(notes_url)\n",
        "\n",
        "df = pd.concat([df_topv2, df_mslatte, df_event, df_notes], ignore_index=True)\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afChBmPMIMFM",
        "outputId": "0683453d-24d3-4193-d331-c60cdbebafaf"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "<ipython-input-16-2afbbd4d8936>:34: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['text'] = df['utterance']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text    label\n",
            "0  That is kind of important. I've thought of tim...  бележка\n",
            "1  The Japanese beetle seems to be the topic of c...  бележка\n",
            "2     Bilingual Storytime/Hora de cuentos bilingï¿½e  събитие\n",
            "3  I do need to fix my schedule, this whole \"free...  бележка\n",
            "4  Extreme timing how it all works out.  My mum a...  бележка\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop_duplicates(inplace=True)\n",
        "df.dropna(subset=['text'], inplace=True)"
      ],
      "metadata": {
        "id": "egea9JI4qNiv"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['label'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsuXQu4hU4nR",
        "outputId": "438c1b9f-b63c-4a13-c22d-2340e16d2c9b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label\n",
            "бележка      616062\n",
            "събитие       22289\n",
            "напомняне     17285\n",
            "задача         9997\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "min_count = df['label'].value_counts().min()\n",
        "\n",
        "df = df.groupby('label', group_keys=False).apply(lambda x: x.sample(n=min_count, random_state=42))\n",
        "\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "print(df['label'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4l8tdOKU82o",
        "outputId": "032491ae-c9a9-4cbb-b8c4-a3900753e41f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label\n",
            "бележка      9997\n",
            "задача       9997\n",
            "напомняне    9997\n",
            "събитие      9997\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-91e3ce79fc41>:3: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df = df.groupby('label', group_keys=False).apply(lambda x: x.sample(n=min_count, random_state=42))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prepare data for pytorch\n"
      ],
      "metadata": {
        "id": "xnzYW_QvaSU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=10000)\n",
        "X_tfidf = vectorizer.fit_transform(df['text']).toarray()\n",
        "\n",
        "df['label_encoded'] = pd.Categorical(df['label']).codes\n",
        "y = df['label_encoded'].values\n",
        "label_mapping = dict(enumerate(pd.Categorical(df['label']).categories))\n",
        "\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X_tfidf, y, test_size=0.15, random_state=42, stratify=y\n",
        ")\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val, test_size=0.15, random_state=42, stratify=y_train_val\n",
        ")\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "ymckyic6aSKq"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Classifier"
      ],
      "metadata": {
        "id": "hVWD7XnVZ5Qb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultinomialLogisticRegression(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(MultinomialLogisticRegression, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.linear(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "SrunyL8IVHjT"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "input_dim = X_train_tensor.shape[1]\n",
        "num_classes = len(np.unique(y))\n",
        "model = MultinomialLogisticRegression(input_dim, num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "V7k5o4uCjwfO"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, loader, val_X, val_y, criterion, optimizer, n_epochs=50, patience=5):\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "    best_model_state = None\n",
        "\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(n_epochs):\n",
        "        total_loss = 0\n",
        "        for batch_X, batch_y in loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item() * batch_X.size(0)\n",
        "        train_loss = total_loss / len(loader.dataset)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_logits = model(val_X)\n",
        "            val_loss = criterion(val_logits, val_y).item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{n_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "          best_val_loss = val_loss\n",
        "          best_mode_state = model.state_dict()\n",
        "          epochs_no_improve = 0\n",
        "        else:\n",
        "          epochs_no_improve += 1\n",
        "\n",
        "          if epochs_no_improve >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "TG7bjDTHj_3H"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, train_loader, X_val_tensor, y_val_tensor, criterion, optimizer, n_epochs=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilR1H_ppkG0Z",
        "outputId": "d872a6e7-197a-4ba4-cb99-291c1a6add14"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30, Train Loss: 1.0905, Val Loss: 0.8686\n",
            "Epoch 2/30, Train Loss: 0.7379, Val Loss: 0.6279\n",
            "Epoch 3/30, Train Loss: 0.5509, Val Loss: 0.4881\n",
            "Epoch 4/30, Train Loss: 0.4359, Val Loss: 0.3984\n",
            "Epoch 5/30, Train Loss: 0.3590, Val Loss: 0.3364\n",
            "Epoch 6/30, Train Loss: 0.3042, Val Loss: 0.2912\n",
            "Epoch 7/30, Train Loss: 0.2632, Val Loss: 0.2570\n",
            "Epoch 8/30, Train Loss: 0.2316, Val Loss: 0.2303\n",
            "Epoch 9/30, Train Loss: 0.2064, Val Loss: 0.2090\n",
            "Epoch 10/30, Train Loss: 0.1859, Val Loss: 0.1915\n",
            "Epoch 11/30, Train Loss: 0.1690, Val Loss: 0.1772\n",
            "Epoch 12/30, Train Loss: 0.1547, Val Loss: 0.1651\n",
            "Epoch 13/30, Train Loss: 0.1426, Val Loss: 0.1549\n",
            "Epoch 14/30, Train Loss: 0.1322, Val Loss: 0.1461\n",
            "Epoch 15/30, Train Loss: 0.1232, Val Loss: 0.1387\n",
            "Epoch 16/30, Train Loss: 0.1153, Val Loss: 0.1321\n",
            "Epoch 17/30, Train Loss: 0.1084, Val Loss: 0.1264\n",
            "Epoch 18/30, Train Loss: 0.1023, Val Loss: 0.1215\n",
            "Epoch 19/30, Train Loss: 0.0968, Val Loss: 0.1171\n",
            "Epoch 20/30, Train Loss: 0.0919, Val Loss: 0.1132\n",
            "Epoch 21/30, Train Loss: 0.0875, Val Loss: 0.1098\n",
            "Epoch 22/30, Train Loss: 0.0835, Val Loss: 0.1068\n",
            "Epoch 23/30, Train Loss: 0.0799, Val Loss: 0.1041\n",
            "Epoch 24/30, Train Loss: 0.0766, Val Loss: 0.1017\n",
            "Epoch 25/30, Train Loss: 0.0736, Val Loss: 0.0995\n",
            "Epoch 26/30, Train Loss: 0.0708, Val Loss: 0.0975\n",
            "Epoch 27/30, Train Loss: 0.0683, Val Loss: 0.0958\n",
            "Epoch 28/30, Train Loss: 0.0659, Val Loss: 0.0943\n",
            "Epoch 29/30, Train Loss: 0.0638, Val Loss: 0.0928\n",
            "Epoch 30/30, Train Loss: 0.0618, Val Loss: 0.0915\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialLogisticRegression(\n",
              "  (linear): Linear(in_features=10000, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_logits = model(X_test_tensor)\n",
        "    predicted_classes = torch.argmax(test_logits, dim=1)\n",
        "    accuracy = (predicted_classes == y_test_tensor).float().mean().item()\n",
        "    print(\"Test accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBqdUNUXkfVX",
        "outputId": "2666828f-7151-4419-af3d-c9312f45ce7f"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.9764961004257202\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJONOrB_7vqk",
        "outputId": "6a3b82be-ef08-4b68-b860-15ee127f5c6c"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5999])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "y_test_np = y_test_tensor.cpu().numpy()\n",
        "predicted_classes_np = predicted_classes.cpu().numpy()\n",
        "\n",
        "accuracy = accuracy_score(y_test_np, predicted_classes_np)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "report = classification_report(y_test_np, predicted_classes_np)\n",
        "print(\"\\nClassification Report:\\n\", report)\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test_np, predicted_classes_np)\n",
        "print(\"\\nConfusion Matrix:\\n\", conf_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PS6yJ2f6x1i",
        "outputId": "04c901e3-c64b-476c-8662-b07fece8314f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9764960826804467\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98      1500\n",
            "           1       0.95      0.98      0.96      1499\n",
            "           2       0.99      1.00      1.00      1500\n",
            "           3       0.98      0.96      0.97      1500\n",
            "\n",
            "    accuracy                           0.98      5999\n",
            "   macro avg       0.98      0.98      0.98      5999\n",
            "weighted avg       0.98      0.98      0.98      5999\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[1453   34    3   10]\n",
            " [   3 1472    2   22]\n",
            " [   0    3 1495    2]\n",
            " [  12   47    3 1438]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_texts = [\n",
        "    \"Today was a great day, the birds were chirping\",\n",
        "    \"Buy groceries\",\n",
        "    \"remind me to wish Sam a happy birthday this Thursday\",\n",
        "    \"Meeting with the team next week\",\n",
        "    \"I need to finish my homework tomorrow\"\n",
        "]\n",
        "\n",
        "x_new = vectorizer.transform(new_texts).toarray()\n",
        "x_new_tensor = torch.tensor(x_new, dtype=torch.float32)\n",
        "\n",
        "with torch.no_grad():\n",
        "  logits = model(x_new_tensor)\n",
        "  probabilities = F.softmax(logits, dim=1)\n",
        "  predicted_classes = torch.argmax(logits, dim=1)\n",
        "\n",
        "for text, predicted_class, prob in zip(new_texts, predicted_classes, probabilities):\n",
        "    confidence = prob[predicted_class.item()].item()\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Predicted Class: {label_mapping[predicted_class.item()]}\")\n",
        "    print(f\"Confidence: {confidence}\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CH9zcfyVLze",
        "outputId": "e51c3ca8-4ba8-4a6e-ed5b-c2f22ae26c8d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Today was a great day, the birds were chirping\n",
            "Predicted Class: бележка\n",
            "Confidence: 0.9337354898452759\n",
            "\n",
            "\n",
            "Text: Buy groceries\n",
            "Predicted Class: задача\n",
            "Confidence: 0.9831216335296631\n",
            "\n",
            "\n",
            "Text: remind me to wish Sam a happy birthday this Thursday\n",
            "Predicted Class: напомняне\n",
            "Confidence: 0.986918032169342\n",
            "\n",
            "\n",
            "Text: Meeting with the team next week\n",
            "Predicted Class: събитие\n",
            "Confidence: 0.42145293951034546\n",
            "\n",
            "\n",
            "Text: I need to finish my homework tomorrow\n",
            "Predicted Class: напомняне\n",
            "Confidence: 0.8711926937103271\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Translator\n"
      ],
      "metadata": {
        "id": "bn5u5OboZ8uO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai-whisper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_7RxCmrln6f",
        "outputId": "a68585c0-6384-4989-a1f9-efd1d0129ed4"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.11/dist-packages (20240930)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.61.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.5.1+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.6.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.8.0)\n",
            "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from triton>=2.0.0->openai-whisper) (3.17.0)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.44.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2024.9.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "audio_model = whisper.load_model(\"base\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omViR6ZVmdTb",
        "outputId": "6247cd90-bbbe-4b27-ba90-410ac55fcb9c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def transcribe_and_classify(audio_path, whisper_model, vectorizer, model, language=\"en\"):\n",
        "    result = whisper_model.transcribe(audio_path, language=language)\n",
        "    transcribed_text = result[\"text\"]\n",
        "    print(\"Transcribed text:\")\n",
        "    print(transcribed_text)\n",
        "\n",
        "    X_new = vectorizer.transform([transcribed_text]).toarray()\n",
        "    X_new_tensor = torch.tensor(X_new, dtype=torch.float32)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(X_new_tensor)\n",
        "        predicted_class = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    predicted_label = label_mapping[predicted_class]\n",
        "    print(\"Classification:\", predicted_label)\n",
        "    return transcribed_text, predicted_label"
      ],
      "metadata": {
        "id": "HNC8iJl-m7ZY"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_file = '/content/drive/MyDrive/Colab Notebooks/data/recording.m4a'\n",
        "transcribe_and_classify(audio_file, audio_model, vectorizer, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zgsvInylpfT",
        "outputId": "be1cedc1-ddc3-4948-97e7-337225989c61"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcribed text:\n",
            " Mind me to clean the dishes tomorrow.\n",
            "Classification: напомняне\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(' Mind me to clean the dishes tomorrow.', 'напомняне')"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "audio_file = '/content/drive/MyDrive/Colab Notebooks/data/AIRecording2.m4a'\n",
        "transcribe_and_classify(audio_file, audio_model, vectorizer, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjaNKbOjn8YX",
        "outputId": "1659fd5b-aa90-4953-8ad6-e2b175d5e4e4"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcribed text:\n",
            " Hangout with friends this Saturday.\n",
            "Classification: събитие\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(' Hangout with friends this Saturday.', 'събитие')"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "audio_file = '/content/drive/MyDrive/Colab Notebooks/data/AIRecording3.m4a'\n",
        "transcribe_and_classify(audio_file, audio_model, vectorizer, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksUiG9s8n8tq",
        "outputId": "e5e25bb6-35a7-45b9-af7d-a1311814b6da"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcribed text:\n",
            " Today I made with friends and we had a great time. It was a pleasant experience. I would do it again.\n",
            "Classification: бележка\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(' Today I made with friends and we had a great time. It was a pleasant experience. I would do it again.',\n",
              " 'бележка')"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XWvuPxITr4Dl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import torch\n",
        "\n",
        "# --- Save the TF-IDF vectorizer and label mapping using joblib ---\n",
        "joblib.dump(vectorizer, \"vectorizer.joblib\")\n",
        "joblib.dump(label_mapping, \"label_mapping.joblib\")\n",
        "print(\"Vectorizer and label mapping saved.\")\n",
        "\n",
        "# --- Save the PyTorch model's state dictionary ---\n",
        "torch.save(model.state_dict(), \"classifier_model.pt\")\n",
        "print(\"Classifier model saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ys2iTblpz6dh",
        "outputId": "47713028-1a7e-4d06-9f26-4482f38d4c14"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectorizer and label mapping saved.\n",
            "Classifier model saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Server"
      ],
      "metadata": {
        "id": "wrg9g9K6r5VM"
      }
    }
  ]
}